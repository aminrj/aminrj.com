{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Hands-on DevSecOps","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"AI/getting-started-with-langchain/","title":"Getting started with LangChain","text":"<p>LangChain helps you develop applications powered by language models such as ChatGPT. </p> <p>In this video we will start by installing LangChain and set up a new environment to build a simple application with langChain.</p> <p>We will go through and explain the most basic common components of LangChain such as prompt templates, models and output parsers.</p> <p>This tutorial is intended for complete beginners, so don\u2019t worry, just stick around, we will explain all these concepts and demonstrate how to use them.</p>"},{"location":"AI/getting-started-with-langchain/#installation","title":"Installation","text":""},{"location":"AI/getting-started-with-langchain/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>First, let\u2019s quickstart our environment using the Python Jupyter Notebook.</p> <p>Jupyter notebook is a web application for creating and running python programs on your browser.</p>"},{"location":"AI/getting-started-with-langchain/#ollama","title":"OLLAMA","text":"<p>Ollama allows you to run open-source large language models, such as Llama 2, locally. First, follow these instructions to set up and run a local Ollama instance:</p> <ul> <li>Download</li> <li>Fetch a model via <code>llama pull llama2</code></li> </ul> <pre><code>from langchain_community.llms import Ollama\nllm = Ollama(model=\"llama2\")\n</code></pre>"},{"location":"cloud-native/1-introduction-to-CN-technology/","title":"Introduction to Cloud-Native Technology","text":""},{"location":"cloud-native/1-introduction-to-CN-technology/#presentation-of-the-setup","title":"Presentation of the setup","text":"<p>In this walkthrough, we will create an application that uses the main cloud-native components to illustrate their use in a cloud-native application development process.</p> <p>This is the first first setup of our example that we will be using throughout the course, so if some concepts will be briefly introduced to get the tutorial up and running.</p> <p>The detailed explanation will be provided throughout the upcoming sections.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#containerizing-apps","title":"Containerizing apps","text":""},{"location":"cloud-native/1-introduction-to-CN-technology/#install-docker","title":"Install Docker","text":"<p>Containerizing your application, that is pachaging it and its dependencies into an executable container is a required step for adopting Cloud-Native. This will allow your applicatoin to run anywhere without needing to install those dependencies on the host machine.</p> <p>The first step is to install Docker. Docker is distribued as a developer tool that is available for most platforms as Docker Desktop. You can check the link (https://www.docker.com/products/docker-desktop) for platform spefic installations.</p> <p>I have it already installed on my machine so I won't go through the installation process here.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#running-commands-in-docker","title":"Running commands in Docker","text":"<p>To explore how docker works before we build our own application containers, we can bring up a containerized Linux shell in Docker, like so:</p> <pre><code>docker run -it ubuntu bash\n</code></pre> <p>This downloads the base ubuntu image, start a container, and run the bash command against it. The <code>-it</code> parameters make it an interactive bash terminal. This means that now, we are in the container and anything we run will happen in the container.</p> <pre><code>apt-get update\napt-get install -y python3\npython3\n&gt;&gt;&gt; print (\"Hello Cloud-Native\")\nHello Cloud-Native\n&gt;&gt;&gt; exit()\n#\n</code></pre> <p>To leave the container, we type the command <code>exit</code>. We just installed Python on the container without altering the local system.</p> <p>To start the container again, we run <code>docker ps -a</code> to get the container ID. Then, we start it with <code>docker start $CONTAINER_ID</code> to boot it, and <code>docker attach $CONTAINER_ID$</code> to reconnect our shell:</p> <pre><code>docker ps -a\nCONTAINER_ID=xxxxxxxxx\ndocker start $CONTAINER_ID\ndocker attach $CONTAINER_ID\nexit\n</code></pre> <p>To clean up the list of stoped containers type the command <code>docker system prune -a</code>.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#building-our-own-images","title":"Building our own images","text":"<p>Now that we started a Linux container, installed Python and create a simple Python script that ran in the container, let's say that we want to make this repeatable. We want to capture the configuration of the container (installing Python and our application) in our own container image. This simple example use only a simple Python script, however, you can imaging that the application can be anything you want it to be.</p> <p>This process uses a configuration file known as <code>Dockerfile</code>. It is a set of procedural instructions used to build your container like bash script that configure a VM with your app and its dependencies. The only difference is that the output is a container image.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#create-a-basic-python-app","title":"Create a basic Python app","text":"<p>Our app is a simple Python script:</p> hello.py<pre><code>print(\"Hello Cloud-Native\")\n</code></pre> <p>We need to create a Dockerfile, pick a base container image to use as the starting point, configure Python, and add the program:</p> <pre><code>FROM ubuntu\nRUN apt-get update\nRUN apt-get install -y python3\nCOPY . /app\nWORKDIR /app \n</code></pre> <p>Build the container and name (tag) it hello. Once built, we can the python script like so</p> <pre><code>docker build . -t hello\ndocker run hellp python3 hellp.py\nHello Cloud-Native\n</code></pre> <p>Here the command are the same like we just did before, here rather than starting from ubuntu container image directly, we use it as our base image for the Dockerfile. Then we run the same apt-get commands in the container.</p> <p>Now, we encapsulated the environment we built and our script into a package that we can use and run ourselves or share with others.</p> <p>Compare this to installing Python on your developer machine and running everything locally. For python, this might not be a problem, but imagine a more complicated application with several tools and libraries requiring specific versions and dependencies.</p> <p>Containers solve this problem by isolating applications along with their dependencies in their own container images.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#adding-a-default-command","title":"Adding a default command","text":"<p>The command executed in the container <code>python3 hello.py</code> is the same each time. Rather than repeating it each time, we can specify that in the Dockerfile:</p> <pre><code>...\nCMD python3 hello.py\n</code></pre> <p>This command doesn't change the way the container is built, it only specify the default command that will be executed when you call <code>docker run</code>. This doesn't stop you from overriding it and executing a different command at run time.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#adding-dependencies","title":"Adding dependencies","text":"<p>Most applications will need their dependencies to be installed on the base image. To do so, you install dependencies during the container build process. The same way we added Python on ubuntu base image before, we can install all dependencies needed by running commands on the base image:</p> <pre><code>FROM python:3\nRUN apt-get update\nRUN apt-get install -y mariadb-client COPY . /app\nWORKDIR /app\nCMD python3 hello.py\n</code></pre> <p>Here we start from a more specific base image that contain everything needed to run Python programs, updated it and installed the mariadb-client on it as an additional dependency.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#running-containers-on-kubernetes","title":"Running containers on Kubernetes","text":"<p>Now that we have our application containerized, we need a kubernetes cluster to deploy our application the Cloud-Native way. For very simple deployments, we can deploy one container per VM and scale our VMs as needed. That way, we have the advantage of containers such as convinient packaging without the complexity of having a Kubernetes cluster.</p> <p>However, you most likely will have a number of different services to deploy, so we need something more flexible. This is where a container orchestrator such as Kubernetes becomes handy. It contains all the tooling that handle the scheduling and monitoring of different containers on different machines.</p> <p>With an orchestrator such as Kubernetes, we can adopt the microservices patterns where various parts of your application are deployed and managed separately.</p> <p>We will see this in more details later in the course. For now, lets get us a running Kubernetes cluster to deploy our freshly built container.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#creating-a-cluster","title":"Creating a cluster","text":"<p>Minikube is a great choice for testing locally and allows you to test with a multinode environment. It is maintained by the open source Kubernetes community. Follow the link https://minikube.sigs .k8s.io/docs/start/ to install Minikube for your system.</p> <pre><code>minikube start --nodes 2\nkubectl get nodes\n</code></pre> <p>The <code>start</code> command will automatically configure <code>kubectl</code> to use Minikube context. In other words, any <code>kubectl</code> command will operate on the Minikube cluster.</p>"},{"location":"cloud-native/1-introduction-to-CN-technology/#uploading-your-container","title":"Uploading your container","text":"<p>Up until now, the container we have created have been stored and run locally on your machine. Before you can deploy the container into Kubernetes, you will need to upload your container image data and provide a way for Kubernetes to fetch the image.</p> <p>For this, we use what we call a container registery. Docker Hub is a popular choice as a container registery, particularly when it comes to public container images. You will need a Docker Hub account to start uploading images to the registery. Go to the Docker Hub website and create a free account.</p> <p>Once the repository is setup, Docker is authenticated and your image is tagged, you can push the image to the repository with:</p> <pre><code>docker push $IMAGE_TAG\n</code></pre>"},{"location":"cloud-native/1-introduction-to-CN-technology/#deploying-to-kubernetes","title":"Deploying to Kubernetes","text":"<p>With our cluster ready and kubectl configured for us, we can deploy our first application. For this, we need a <code>deployment</code> object that Kubernetes will create from a declarative configuration. This means that you declare the specification wanted, like 'I need 2 copies of my python application running in the cluster' and Kubernetes will strive to meet the requirement you specified.</p> <p>We will dive deeper into Kubernetes core concept in the next chapter. For now, here is the deployment code:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      pod: hello-cloud-native\n  template:\n    metadata:\n      labels:\n        pod: hello-cloud-native-pod\n    spec:\n      containers:\n      - name: hello-container\n        image: docker.io/aminrj/hello:1\n</code></pre> <p>This manifest will create two replicas of our python application after we run the <code>kubectl create -f deploy.yaml</code> command. To check the result, we run <code>kubectl get deploy</code> command. To see the pods created for this deployement we type <code>kubectl get pods</code>.</p> <p>This is it, we created our first container, build the image, uploaded it to the registery and deployed it on our local kubernetes cluster as a deployement.</p> <p>In the next section of the course, we will dive deeper into the core concepts of Kubernetes to learn how Kubernetes makes cloud-native application deployements more flexible.</p>"},{"location":"k8s/argocd-deployment-patterns/","title":"ArgoCD","text":"<p>Kubernetes has emerged as the de-facto platform for modern containerized applications. </p> <p>However, managing the deployment, configuration, and lifecycle of applications on Kubernetes can be a complex and error-prone task.</p> <p>This is where ArgoCD, a declarative GitOps continuous delivery tool for Kubernetes, comes into play.</p> <p>Here we explore how ArgoCD deploy applications into Kubernetes cluster and explain the differences between different deployment patterns.</p>"},{"location":"k8s/argocd-deployment-patterns/#why-argocd","title":"Why ArgoCD","text":"<p>ArgoCD provides a centralized repository for defining the desired state of applications, ensuring that the actual state of the cluster matches the desired state.</p> <p>This eliminates the need for manual intervention and reduces the risk of human error. It also offers some benefits that we can list as follow:</p> <ol> <li>Declarative GitOps: ArgoCD adheres to the GitOps philosophy, using Git repositories as the single source of truth for application configurations. This promotes collaboration and visibility among development teams.</li> <li>Automated Reconciliation: ArgoCD continuously monitors the differences between the desired application state defined in Git and the actual state of the cluster. It automatically reconciles these differences, applying any necessary changes to ensure consistency.</li> <li>Centralized Configuration Management: ArgoCD provides a centralized platform for managing application configurations, eliminating the need for multiple configuration repositories scattered across different teams or environments.</li> <li>Improved Release Management: ArgoCD simplifies the release management process by automating deployments, rollbacks, and canary deployments. This reduces the risk of downtime and disruptions.</li> </ol>"},{"location":"k8s/argocd-deployment-patterns/#what-is-a-deployment-pattern","title":"What is a deployment pattern","text":"<p>An ArgoCD Application serves as a representation of the collection of Kubernetes-native manifests (usually YAML) that form the core components of an application. This Custom Resource Definition (CRD) defines the source type, which specifies the deployment tool (Helm or Git) and the location of those manifests. However, this approach presents certain challenges:</p> <ol> <li>Bootstrapping: Deploying GitOps applications in a \"GitOps-friendly\" manner can be a hurdle. How does one deploy the ArgoCD Application CR manifest in a GitOps way, particularly during the initial setup phase (day 0)?</li> <li>Source Type Limitations: An ArgoCD Application can only have a single source type defined (though it might change in the coming versions). This restriction prevents the combination of Helm charts and YAML manifests within a single application definition.</li> </ol> <p>To address these challenges, users have employed a workaround: creating an ArgoCD Application that deploys other ArgoCD Applications. This approach offers several advantages:</p> <ol> <li>Massive Deployments: It enables the deployment of multiple applications simultaneously. Instead of deploying numerous ArgoCD Application objects individually, a single application can manage the deployment of all the others.</li> <li>Logical Application Grouping: It facilitates the logical grouping of real-world applications comprising YAML manifests and Helm charts within ArgoCD.</li> <li>Convenient Monitoring: It provides a centralized \"watcher\" application that monitors the deployment and health of all the underlying applications.</li> </ol> <p>The \"App of Apps\" pattern, mentioned in the original text, exemplifies this approach. It provides a practical solution for managing complex application landscapes and simplifying deployment processes. For further details, refer to the Argo Project website for comprehensive information on the \"App of Apps\" pattern. (App of Apps from the Argo Project website.)</p>"},{"location":"k8s/argocd-deployment-patterns/#argocd-applicationsets","title":"ArgoCD ApplicationSets","text":"<p>ArgoCD ApplicationSets, represent a significant advancement of the \"App of Apps\" deployment pattern. This enhancement builds upon the existing concept of managing multiple applications within a single application manifest, introducing greater flexibility and addressing a broader spectrum of use cases. The ApplicationSets controller operates as a separate entity, complementing the functionality of the ArgoCD Application CRD.</p> <p>Unlike traditional \"App of Apps\" manifests, which are static and require manual updates, ArgoCDApplicationSets offer dynamic capabilities. They can automatically generate application manifests based on templates or Git repositories, enabling the management of complex application landscapes with ease. This approach eliminates the need to manually define and manage individual application manifests.</p>"},{"location":"k8s/argocd-deployment-patterns/#why-use-applicationsets","title":"Why use ApplicationSets","text":"<p>ArgoCD ApplicationSets also simplify the deployment and management of multi-cluster applications.</p> <p>They can simultaneously manage the deployment and configuration of applications across multiple Kubernetes clusters, ensuring consistency and reliability. This feature is particularly beneficial for organizations with geographically dispersed or hybrid cloud environments.</p> <p>The ApplicationSet\u00a0controller\u00a0 is made up of \u201cgenerators\u201d. These \u201cgenerators\u201d instruct the ApplicationSet how to generate Applications by the provided repo or repos, and it also instructs where to deploy the Application. There are 3 \u201cgenerators\u201d that I will be exploring are:</p> <ul> <li>List Generator</li> <li>Cluster Generator</li> <li>Git Generator</li> </ul> <p>Each \u201cgenerator\u201d tackles different scenarios and use cases. Every \u201cgenerator\u201d gives you the same end result: Deployed ArgoCD Applications that are loosely coupled together for easy management. What you use would depend on a lot of factors like the number of clusters managed, git repo layout, and environmental differences.</p>"},{"location":"k8s/argocd-deployment-patterns/#hands-on-tutorial","title":"Hands on tutorial","text":"<p>I made a video tutorial using both ArgoCD deployment patterns presented in this article.</p> <p>TODO : Link to the YouTube video</p>"},{"location":"k8s/argocd-deployment-patterns/#conclusion","title":"Conclusion","text":"<p>ArgoCD is a powerful tool that simplifies and automates application deployments on Kubernetes. By adopting a declarative GitOps approach, ArgoCD ensures that the actual state of the cluster matches the desired state defined in Git, reducing the risk of downtime, inconsistencies, and human errors. The \"app of apps\" pattern and the Applicationsets controller offer different ways to define and manage application groups, providing flexibility and control for managing complex application landscapes. As Kubernetes adoption continues to grow, ArgoCD is poised to become an indispensable tool for organizations that want to streamline their application delivery pipelines and achieve continuous delivery excellence.</p> <p>Overall, ArgoCD ApplicationSets provide a powerful and versatile tool for managing application groups in Kubernetes. They address the limitations of the traditional \"App of Apps\" pattern and offer a more comprehensive solution for managing complex application landscapes. With their dynamic nature, multi-cluster support, and automated generation capabilities, ArgoCD ApplicationSets are poised to become a go-to choice for organizations seeking to streamline their application delivery processes.</p>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/","title":"Continuous Kubernetes Security Scans with Kubescape","text":""},{"location":"k8s/continuous-security-monitoring-with-kubescape/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Minikube cluster.</li> <li>Terraform installed on your system.</li> <li>Basic understanding of Kubernetes concepts and the use of <code>kubectl</code>.</li> </ul> <p>If you are new to the topic, consider reading this article bellow that would help you with the prerequisites:</p> <p> TODO: Add article link</p>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#steps","title":"Steps","text":""},{"location":"k8s/continuous-security-monitoring-with-kubescape/#1-set-up-terraform","title":"1. Set up Terraform","text":"<ul> <li>Create a new directory for your project.</li> <li>Create a file named <code>main.tf</code> with the following Terraform configuration:</li> </ul> <pre><code># Initilize terraform providers\nprovider \"kubernetes\" {\nconfig_path = \"~/.kube/config\"\n}\n\nprovider \"helm\" {\nkubernetes {\n    config_path = \"~/.kube/config\"\n}\n}\n</code></pre> <ul> <li> <p>Initialize Terraform:</p> <pre><code>terraform init\n</code></pre> </li> </ul>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#2-deploy-grafana-and-prometheus","title":"2. Deploy Grafana and Prometheus","text":"<pre><code># Create a namespace for observability\nresource \"kubernetes_namespace\" \"observability-namespace\" {\n    metadata {\n        name = \"observability\"\n    }\n}\n\n# Prometheus setup\nresource \"helm_release\" \"prometheus\" {\n    name       = \"prometheus\"\n    repository = \"https://prometheus-community.github.io/helm-charts\"\n    chart      = \"prometheus\"\n    version    = \"25.8.2\"\n    namespace  = \"observability\"\n\n    depends_on = [kubernetes_namespace.observability-namespace]\n}\n\n# Grafana setup\nresource \"helm_release\" \"grafana\" {\n    name       = \"grafana\"\n    repository = \"https://grafana.github.io/helm-charts\"\n    chart      = \"grafana\"\n    version    = \"7.1.0\"\n    namespace  = \"observability\"\n\n    values     = [file(\"${path.module}/values/grafana.yaml\")]\n    depends_on = [kubernetes_namespace.observability-namespace]\n} \n</code></pre> <p>Before hitting <code>terraform apply</code>, you need to add the Grafana configuration in the <code>values</code> folder:</p> values/grafana.yaml<pre><code>persistence.enabled: true\npersistence.size: 10Gi\npersistence.existingClaim: grafana-pvc\npersistence.accessModes[0]: ReadWriteOnce\npersistence.storageClassName: standard\n\nadminUser: admin\nadminPassword: grafana\n\ndatasources: \n datasources.yaml:\n   apiVersion: 1\n   datasources:\n    # configure Prometheus datasource\n    - name: Prometheus\n      type: prometheus\n      access: proxy\n      orgId: 1\n      url: http://prometheus-server.observability.svc.cluster.local\n      basicAuth: false\n      isDefault: false\n      version: 1\n      editable: true\n</code></pre> <p>Here we specify default admin credentials (only for demonstration purpose, do not try this on a real production cluster). We also add the data source for Prometheus so we don't have to add it in the Grafa UI each time we deploy our infrastructure.</p> <ul> <li> <p>Deploy Prometheus and Grafana:</p> <pre><code>terraform apply\n</code></pre> <p>(Type 'yes' when prompted)</p> </li> </ul>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#3-deploy-loki-and-promtail","title":"3. Deploy Loki and Promtail","text":"<p>Since, we will be using Loki to send security logs to Grafana along with Kubernetes audit logs, we configure Loki and Promtail as well:</p> <pre><code># Helm chart for Loki\nresource \"helm_release\" \"loki\" {\n    name       = \"loki\"\n    repository = \"https://grafana.github.io/helm-charts\"\n    chart      = \"loki\"\n    version    = \"5.41.5\"\n    namespace  = \"observability\"\n\n    values     = [file(\"${path.module}/values/loki.yaml\")]\n    depends_on = [kubernetes_namespace.observability-namespace]\n} \n\n# Helm chart for promtail\nresource \"helm_release\" \"promtail\" {\n    name       = \"promtail\"\n    repository = \"https://grafana.github.io/helm-charts\"\n    chart      = \"promtail\"\n    version    = \"6.15.3\"\n    namespace  = \"observability\"\n\n    values     = [file(\"${path.module}/values/promtail.yaml\")]\n    depends_on = [kubernetes_namespace.observability-namespace]\n}\n</code></pre> <p>Same thing here with the configuration of the tools, create the following yaml files in the <code>values</code> folder:</p> values/loki.yaml<pre><code>loki:\n  auth_enabled: false\n  commonConfig:\n    replication_factor: 1\n  storage:\n    type: 'filesystem'\nsingleBinary:\n  replicas: 1\n</code></pre> values/prometheus.yaml<pre><code># Mount folder /var/log from node\nextraVolumes:\n  - name: node-logs\n    hostPath:\n      path: /var/log\n\nextraVolumeMounts:\n  - name: node-logs\n    mountPath: /var/log/host\n    readOnly: true\n\n# Add Loki as a client to Promtail\nconfig:\n  clients:\n    - url: http://loki-gateway.observability.svc.cluster.local/loki/api/v1/push\n\n# Scraping kubernetes audit logs located in /var/log/kubernetes/audit/\n  snippets:\n    scrapeConfigs: |\n      - job_name: audit-logs\n        static_configs:\n          - targets:\n              - localhost\n            labels:\n              job: audit-logs\n              __path__: /var/log/host/kubernetes/**/*.log\n</code></pre> <ul> <li> <p>Deploy Loki and Promtail:</p> <pre><code>terraform apply\n</code></pre> <p>(Type 'yes' when prompted)</p> </li> </ul>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#4-install-kubescape","title":"4. Install Kubescape","text":"<p>Following the same GitOps approach, we will deploy Kubescape the same way we deploy other observability tools, using Terraform to deploy Kubescape operator Helm chart.</p> <pre><code># Install the Kubescape Helm chart\nresource \"helm_release\" \"kubescape\" {\n  name       = \"kubescape\"\n  repository = \"https://kubescape.github.io/helm-charts\"\n  chart      = \"kubescape-operator\"\n  version    = \"1.18.3\"\n  namespace  = \"kubescape\"\n  create_namespace = true\n\n  depends_on = [kubernetes_namespace.observability-namespace]\n\n  set {\n    name  = \"clusterName\"\n    value = \"minikube\" \n  }\n}\n</code></pre> <p>Check that the pods are in a running state:</p> <pre><code>kubectl get po -n kubescape\nNAME                                  READY   STATUS             RESTARTS        AGE\nkubescape-5dc5cb78c6-m8ss7            1/1     Running            0               20s\nkubevuln-65565b7497-tkzjw             1/1     Running            0               20s\noperator-86c99bbf74-8g9xk             1/1     Running            0               13s\nstorage-5bdb8b54ff-lzs58              1/1     Running            0               20s\n</code></pre> <p>This Helm chart deploys the Kubescape-operator that contains both Kubevuln and kubescape.</p> <p>At this point, we can view the results directly using kubectl as follow:</p> <p>View configuration scan summaries:</p> <pre><code>kubectl get workloadconfigurationscansummaries -A\n</code></pre> <p>Detailed reports are also available:</p> <pre><code>kubectl get workloadconfigurationscans -A\n</code></pre> <p>View image vulnerabilities scan summaries:</p> <pre><code>kubectl get vulnerabilitymanifestsummaries -A\n</code></pre> <p>Detailed reports are available with:</p> <pre><code>kubectl get vulnerabilitymanifests -A\n</code></pre> <p>However, we want to use our monitoring setup to visualize our findings.</p>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#4-view-kubescape-data-to-grafana","title":"4. View Kubescape data to Grafana","text":"<p> TODO: Add article link</p> <ul> <li>Check the configurations</li> <li>Take screenshots of the scans</li> <li>Think about fixing sending scan data to Prometheus</li> </ul>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#4-explore-scan-results-using-the-cli-tool","title":"4. Explore scan results using the CLI tool","text":"<pre><code>$ kubescape scan framework nsa\n \u2705  Initialized scanner\n \u2705  Loaded policies\n \u2705  Loaded exceptions\n \u2705  Loaded account configurations\n \u2705  Accessed Kubernetes objects\nControl: C-0017 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (24/24, 71 it/s)\n \u2705  Done scanning. Cluster: minikube\n \u2705  Done aggregating results\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nFramework scanned: NSA\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2510\n\u2502        Controls \u2502 24 \u2502\n\u2502          Passed \u2502 8  \u2502\n\u2502          Failed \u2502 14 \u2502\n\u2502 Action Required \u2502 2  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\nFailed resources by severity:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2510\n\u2502 Critical \u2502 0  \u2502\n\u2502     High \u2502 16 \u2502\n\u2502   Medium \u2502 51 \u2502\n\u2502      Low \u2502 8  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\nRun with '--verbose'/'-v' to see control failures for each resource.\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Severity \u2502 Control name                                \u2502 Failed resources \u2502 All Resources \u2502 Compliance score  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Critical \u2502 Disable anonymous access to Kubelet service \u2502        0         \u2502       0       \u2502 Action Required * \u2502\n\u2502 Critical \u2502 Enforce Kubelet client TLS authentication   \u2502        0         \u2502       0       \u2502 Action Required * \u2502\n\u2502   High   \u2502 Resource limits                             \u2502        12        \u2502      25       \u2502        52%        \u2502\n\u2502   High   \u2502 Host PID/IPC privileges                     \u2502        2         \u2502      25       \u2502        92%        \u2502\n\u2502   High   \u2502 HostNetwork access                          \u2502        1         \u2502      25       \u2502        96%        \u2502\n\u2502   High   \u2502 Privileged container                        \u2502        1         \u2502      25       \u2502        96%        \u2502\n\u2502  Medium  \u2502 Non-root containers                         \u2502        9         \u2502      25       \u2502        64%        \u2502\n\u2502  Medium  \u2502 Allow privilege escalation                  \u2502        5         \u2502      25       \u2502        80%        \u2502\n\u2502  Medium  \u2502 Ingress and Egress blocked                  \u2502        14        \u2502      25       \u2502        44%        \u2502\n\u2502  Medium  \u2502 Automatic mapping of service account        \u2502        12        \u2502      83       \u2502        86%        \u2502\n\u2502  Medium  \u2502 Cluster internal networking                 \u2502        2         \u2502       6       \u2502        67%        \u2502\n\u2502  Medium  \u2502 Linux hardening                             \u2502        7         \u2502      25       \u2502        72%        \u2502\n\u2502  Medium  \u2502 Secret/etcd encryption enabled              \u2502        1         \u2502       1       \u2502        0%         \u2502\n\u2502  Medium  \u2502 Audit logs enabled                          \u2502        1         \u2502       1       \u2502        0%         \u2502\n\u2502   Low    \u2502 Immutable container filesystem              \u2502        7         \u2502      25       \u2502        72%        \u2502\n\u2502   Low    \u2502 PSP enabled                                 \u2502        1         \u2502       1       \u2502        0%         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          \u2502              Resource Summary               \u2502        33        \u2502      208      \u2502      67.51%       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#5-configure-prometheus-for-kubescape-metrics","title":"5. Configure Prometheus for Kubescape Metrics","text":"<ul> <li> <p>Create <code>prometheus-config.yaml</code>:</p> <pre><code>global:\n  scrape_interval: 15s \nscrape_configs:\n  - job_name: 'kubescape'\n    file_sd_configs:\n      - files:\n        - results.txt \n</code></pre> </li> <li> <p>Create Kubernetes ConfigMap:</p> <pre><code>kubectl create configmap prometheus-config --from-file=prometheus-config.yaml -n monitoring\n</code></pre> </li> <li> <p>Update Prometheus Deployment (in <code>main.tf</code>)</p> <pre><code># Inside your 'helm_release' for Prometheus:\nset {\n    name  = \"extraVolumes\"\n    value =  &lt;&lt;EOF\n    - name: config-volume\n      configMap:\n        name: prometheus-config \n    EOF\n  }\n\n  set {\n    name  = \"extraVolumeMounts\"\n    value =  &lt;&lt;EOF\n    - name: config-volume\n      mountPath: /etc/prometheus/prometheus.yml\n      subPath: prometheus.yml \n    EOF\n  }\n</code></pre> </li> <li> <p>Re-apply Terraform:</p> <pre><code>terraform apply\n</code></pre> </li> </ul>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#6-set-up-grafana-dashboard","title":"6. Set up Grafana Dashboard","text":"<ul> <li> <p>Port-forward Grafana:</p> <pre><code>kubectl port-forward -n monitoring service/grafana 3000:80 \n</code></pre> </li> <li> <p>Access Grafana: http://localhost:3000 (Default: \"admin\"/\"admin\")</p> </li> <li>Import a Kubescape dashboard (https://grafana.com/grafana/dashboards/) or create your own.</li> </ul>"},{"location":"k8s/continuous-security-monitoring-with-kubescape/#7-automate-scans-optional","title":"7. Automate Scans (Optional)","text":"<p>Set up a Kubernetes CronJob to execute Kubescape scans regularly.</p>"},{"location":"k8s/kubernetes-risk-analysis-with-kubesec/","title":"Kubernetes risk analysis with KubeSec","text":""},{"location":"k8s/kubernetes-risk-analysis-with-kubesec/#what-is-risk-analysis-for-kubnernetes","title":"What is risk analysis for Kubnernetes","text":""},{"location":"k8s/kubernetes-risk-analysis-with-kubesec/#performing-risk-analysis-with-kubesec","title":"Performing risk analysis with KubeSec","text":""},{"location":"k8s/kubernetes-risk-analysis-with-kubesec/#the-takeaway","title":"The takeaway","text":""},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/","title":"Kafka Cluster install on Kubernetes and Monitoring with Grafana","text":"<p>With Kubernetes, Kafka clusters can be managed declaratively, reducing the operational overhead of setting up Kafka components and keeping them up-to-date. Kubernetes Operators like Strimzi automate complex tasks such as broker configuration, rolling upgrades, and scaling.</p> <p>This tutorial walks through how to:</p> <ul> <li>Deploy a Kafka cluster within Kubernetes using Strimzi Kafka Operator</li> <li>Enable monitoring of usefull Kafka metrics with Prometheus and Grafana</li> </ul>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#deploy-kafka-on-kubernetes-using-minikube","title":"Deploy Kafka on Kubernetes (using Minikube)","text":""},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#a-new-minikube-cluster","title":"A new Minikube cluster","text":"<pre><code>``` bash\n$ minikube start -p kafka-cluster\n\ud83d\ude04  [kafka-cluster] minikube v1.33.1 on Darwin 14.5 (arm64)\n\u2728  Using the docker driver based on existing profile\n\ud83d\udc4d  Starting \"kafka-cluster\" primary control-plane node in \"kafka-cluster\" cluster\n\ud83d\ude9c  Pulling base image v0.0.44 ...\n\ud83d\udd04  Restarting existing docker container for \"kafka-cluster\" ...\n\ud83d\udc33  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: default-storageclass, storage-provisioner\n\ud83c\udfc4  Done! kubectl is now configured to use \"kafka-cluster\" cluster and \"default\" namespace by default\n```\n</code></pre>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#infrastructure-as-code-with-terraform","title":"Infrastructure As Code with Terraform","text":"<p>To ease our deployment, we use Terraform as our Infrastructure As Code tool.</p> <p> Note: on IaC If you\u2019re not familiar with Terraform, don\u2019t be scared, it is just an automation tool that uses code to declare infrastructures. Install Terraform on you machine and run the two terraform commands bellow in the same folder where you put the *.tf files. that\u2019s it.</p> <p>This Terraform code declares the necessary providers, and creates the namespaces before installing the \u201cstrimiz-cluster-operator\u201d helm-chart:</p> strimzi-kakfa.tf<pre><code>    # Initialize terraform providers\n    provider \"kubernetes\" {\n      config_path = \"~/.kube/config\"\n    }\n    provider \"helm\" {\n      kubernetes {\n        config_path = \"~/.kube/config\"\n      }\n    }\n\n    # Create a namespace for observability\n    resource \"kubernetes_namespace\" \"kafka-namespace\" {\n      metadata {\n        name = \"kafka\"\n      }\n    }\n\n    # Create a namespace for observability\n    resource \"kubernetes_namespace\" \"observability-namespace\" {\n      metadata {\n        name = \"observability\"\n      }\n    }\n\n    # Helm chart for Strimzi Kafka\n    resource \"helm_release\" \"strimzi-cluster-operator\" {\n      name = \"strimzi-cluster-operator\"  \n      repository = \"https://strimzi.io/charts/\"\n      chart = \"strimzi-kafka-operator\"\n      version = \"0.42.0\"\n      namespace = kubernetes_namespace.kafka-namespace.metadata[0].name\n      depends_on = [kubernetes_namespace.kafka-namespace]\n    }\n</code></pre> <ul> <li>Apply terraform script to create the namespace and install Strimzi Kafka Operator</li> </ul> <pre><code>  terraform init\n  ...\n  terraform apply --autor-approve\n</code></pre> <ul> <li>Apply kubernetes yamls to create kafka resources:</li> </ul> <pre><code>  kubectl apply -f kafka\n</code></pre> <pre><code>   $ kubectl -n kafka get po \n   NAME                                        READY   STATUS    RESTARTS   AGE \n   strimzi-cluster-operator-6948497896-swlvp   1/1     Running   0          77s\n</code></pre>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#kafka-cluster-with-strimzi","title":"Kafka Cluster with Strimzi","text":"<p>Now that our Strimzi-Kafka-Operator is up and running in our newly created Kubernetes cluster, we create the Kafka cluster by applying the following yaml file with the command :</p> kafka-persistent.yaml<pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: my-cluster\nspec:\n  kafka:\n    version: 3.7.1\n    replicas: 3\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n    config:\n      offsets.topic.replication.factor: 3\n      transaction.state.log.replication.factor: 3\n      transaction.state.log.min.isr: 2\n      default.replication.factor: 3\n      min.insync.replicas: 2\n      inter.broker.protocol.version: \"3.7\"\n    storage:\n      type: jbod\n      volumes:\n      - id: 0\n        type: persistent-claim\n        size: 100Gi\n        deleteClaim: false\n  zookeeper:\n    replicas: 3\n    storage:\n      type: persistent-claim\n      size: 100Gi\n      deleteClaim: false\n  entityOperator:\n    topicOperator: {}\n    userOperator: {}\n</code></pre> <pre><code>kubectl apply -n kafka -f kafka-persistent.yaml\n</code></pre> <p>That\u2019s all that we need to deploy a fully operational Kafka cluster on Kubernetes.</p> <p>In the kafka namespace, we see that our cluster is up and running and that we have 3 replicas of our cluster as well as 3 replicas of the zookeeper:</p> <pre><code>\u279c  $ kubectl -n kafka get po\nNAME                                         READY   STATUS    RESTARTS   AGE\nmy-cluster-entity-operator-5d7c9f484-94zdt   2/2     Running   0          22s\nmy-cluster-kafka-0                           1/1     Running   0          45s\nmy-cluster-kafka-1                           1/1     Running   0          45s\nmy-cluster-kafka-2                           1/1     Running   0          45s\nmy-cluster-zookeeper-0                       1/1     Running   0          112s\nmy-cluster-zookeeper-1                       1/1     Running   0          112s\nmy-cluster-zookeeper-2                       1/1     Running   0          112s\nstrimzi-cluster-operator-6948497896-swlvp    1/1     Running   0          4m9s\n</code></pre> <p>To create Kafka entities (producers, consumers, topics), we use the Kubernetes CRD installed by the Strimzi Operator to do so.</p> <p>For instance, we create a Kafka topic named <code>my-topic</code> by applying the following yaml code: kafka-topic.yaml<pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaTopic\nmetadata:\n  name: my-topic\n  labels:\n    strimzi.io/cluster: my-cluster\nspec:\n  partitions: 1\n  replicas: 1\n  config:\n    retention.ms: 7200000\n    segment.bytes: 1073741824\n</code></pre></p> <pre><code>\u279c  $ kubectl -n kafka apply -f kafka/kafka-topic.yaml\nkafkatopic.kafka.strimzi.io/my-topic created\n</code></pre> <p>To produce some events to our topic, we run the following command:</p> <pre><code>echo \"Hello KafkaOnKubernetes\" | kubectl -n kafka exec -i my-cluster-kafka-0 -c kafka -- \\\n    bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic\n</code></pre> <p>To test consuming this event, we run:</p> <pre><code>\u279c  kubectl -n kafka exec -i my-cluster-kafka-0 -c kafka -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning\nHello KafkaOnKubernetes\n</code></pre> <p>At this point, our Kafka cluster is up and running and we can already send and receive events between different microservices running within Kubernetes.</p>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#monitoring-our-kafka-cluster-with-grafana","title":"Monitoring our Kafka Cluster with Grafana","text":"<p> Collecting metrics is critical for understanding the health and performance of our Kafka cluster.</p> <p>This is important to identify issues before they become critical and make informed decisions about resource allocation and capacity planning.</p> <p>For this, we will use Prometheus and Grafana to monitor Strimzi.</p> <p>Prometheus consumes metrics from the running pods in your cluster when configured with Prometheus rules. Grafana visualizes these metrics on dashboards, providing better interface for monitoring.</p>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#setting-up-prometheus","title":"Setting-up Prometheus","text":"<p>To deploy the Prometheus Operator to our Kafka cluster, we apply the YAML bundle resources file from the Prometheus CoreOS repository:</p> <pre><code>  curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/bundle.yaml &gt; prometheus-operator-deployment.yaml\n</code></pre> <p>Then we update the namespace with our <code>observability</code> namespace:</p> <pre><code>sed -i '' -e '/[[:space:]]*namespace: [a-zA-Z0-9-]*$/s/namespace:[[:space:]]*[a-zA-Z0-9-]*$/namespace: observability/' prometheus-operator-deployment.yaml\n</code></pre> <p> For Linux, use this command instead:</p> <pre><code>sed -E -i '/[[:space:]]*namespace: [a-zA-Z0-9-]*$/s/namespace:[[:space:]]*[a-zA-Z0-9-]*$/namespace: observability/' prometheus-operator-deployment.yaml\n</code></pre> <p>Then, deploy the Prometheus Operator:</p> <p><pre><code>kubectl -n observability create -f prometheus-operator-deployment.yaml\ncustomresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/prometheusagents.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/scrapeconfigs.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created\ncustomresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created\nclusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created\nclusterrole.rbac.authorization.k8s.io/prometheus-operator created\ndeployment.apps/prometheus-operator created\nserviceaccount/prometheus-operator created\nservice/prometheus-operator created\n</code></pre> </p> <p>Now that we have the operator up and running, we need to create the Prometheus server and configure it to watch for Strimzi CRDs in the <code>kafka</code> namespace.</p> <p>Note here that the name of the namespace must match otherwise Prometheus Operator won't scrap any resources we deploy.</p>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#create-the-podmonitor-objects-for-kafka-resources","title":"Create the PodMonitor objects for Kafka resources","text":"<p>In order to tell Kafka CRDs to expose Prometheus metrics, we must create the PodMonitor objects for the metrics we want to monitor:</p> strimzi-pod-monitor.yaml<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: cluster-operator-metrics\n  labels:\n    app: strimzi\nspec:\n  selector:\n    matchLabels:\n      strimzi.io/kind: cluster-operator\n  namespaceSelector:\n    matchNames:\n      - kafka\n  podMetricsEndpoints:\n  - path: /metrics\n    port: http\n---\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: entity-operator-metrics\n  labels:\n    app: strimzi\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: entity-operator\n  namespaceSelector:\n    matchNames:\n      - kafka\n  podMetricsEndpoints:\n  - path: /metrics\n    port: healthcheck\n---\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: kafka-resources-metrics\n  labels:\n    app: strimzi\nspec:\n  selector:\n    matchExpressions:\n      - key: \"strimzi.io/kind\"\n        operator: In\n        values: [\"Kafka\", \"KafkaConnect\", \"KafkaMirrorMaker\", \"KafkaMirrorMaker2\"]\n  namespaceSelector:\n    matchNames:\n      - kafka\n  podMetricsEndpoints:\n  - path: /metrics\n    port: tcp-prometheus\n    relabelings:\n    - separator: ;\n      regex: __meta_kubernetes_pod_label_(strimzi_io_.+)\n      replacement: $1\n      action: labelmap\n    - sourceLabels: [__meta_kubernetes_namespace]\n      separator: ;\n      regex: (.*)\n      targetLabel: namespace\n      replacement: $1\n      action: replace\n    - sourceLabels: [__meta_kubernetes_pod_name]\n      separator: ;\n      regex: (.*)\n      targetLabel: kubernetes_pod_name\n      replacement: $1\n      action: replace\n    - sourceLabels: [__meta_kubernetes_pod_node_name]\n      separator: ;\n      regex: (.*)\n      targetLabel: node_name\n      replacement: $1\n      action: replace\n    - sourceLabels: [__meta_kubernetes_pod_host_ip]\n      separator: ;\n      regex: (.*)\n      targetLabel: node_ip\n      replacement: $1\n      action: replace\n</code></pre> <p>Then we create the Prometheus object and configure it to look for all pods with the labels <code>app: strimzi</code></p> <pre><code>\u279c  kubectl -n observability apply -f strimzi-pod-monitor.yaml\npodmonitor.monitoring.coreos.com/cluster-operator-metrics created\npodmonitor.monitoring.coreos.com/entity-operator-metrics created\npodmonitor.monitoring.coreos.com/bridge-metrics created\npodmonitor.monitoring.coreos.com/kafka-resources-metrics created\n</code></pre> <p>Note that for this to work, we also need the corresponding ServiceAccount, and RBAC objects as follow: prometheus.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus-server\n  labels:\n    app: strimzi\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n      - nodes/proxy\n      - services\n      - endpoints\n      - pods\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups:\n      - extensions\n    resources:\n      - ingresses\n    verbs: [\"get\", \"list\", \"watch\"]\n  - nonResourceURLs: [\"/metrics\"]\n    verbs: [\"get\"]\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus-server\n  labels:\n    app: strimzi\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus-server\n  labels:\n    app: strimzi\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus-server\nsubjects:\n  - kind: ServiceAccount\n    name: prometheus-server\n    namespace: observability\n\n---\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  serviceAccountName: prometheus-server\n  podMonitorSelector:\n    matchLabels:\n      app: strimzi\n  serviceMonitorSelector: {}\n  resources:\n    requests:\n      memory: 400Mi\n  enableAdminAPI: false\n</code></pre></p> <p>Then:</p> <pre><code>\u279c  kubectl -n observability create -f observability/prometheus-install/prometheus.yaml\nclusterrole.rbac.authorization.k8s.io/prometheus-server created\nserviceaccount/prometheus-server created\nclusterrolebinding.rbac.authorization.k8s.io/prometheus-server created\nprometheus.monitoring.coreos.com/prometheus created\n</code></pre>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#configuring-our-kafka-cluster-to-expose-metrics","title":"Configuring our Kafka cluster to expose metrics","text":"<p>To enable and expose metrics in Strimzi for Prometheus, we use metrics configuration properties using the <code>metricsConfig</code> configuration property or our Kafka cluster.</p> <pre><code>    metricsConfig:\n      type: jmxPrometheusExporter\n      valueFrom:\n        configMapKeyRef:\n          name: kafka-metrics\n          key: kafka-metrics-config.yml\n</code></pre> <p>Once configured, we apply the new config which will restart our cluster with the updated configuration:</p> <p>For the configured kafka yaml file, you can find an example in the Strimi examples folder \u201ckafka-metrics.yaml\u201d. Otherwise, you can always refer to the git repository of this project referenced below.</p> <pre><code>kubectl apply -f kafka-metrics.yaml -n kafka\n</code></pre> <p> After running this command, Kafka Zookeeper pods start restarting one by one followed by the Kafka brokers pods until all the pods are running the updated configuration. Hence the rolling upgrade of our Kafka cluster with zero-down-time powered by Kubernetes. (check in the figure below the age value of the my-cluster-kafka-2 compared to the other two, it will be terminated then restarted last).</p> <p></p> <p>For more details on how monitor Strimzi Kafka using Prometheus and Grafana, check the Strimzi documentation.</p>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#deploy-and-configure-grafana","title":"Deploy and Configure Grafana","text":"<p>At this point, our Kafka cluster is up and running and exposes metrics for Prometheus to collect.</p> <p>Now we need to install Grafana using the <code>grafana.yaml</code> file then configure the our Prometheus as data source.</p> <pre><code>kubectl -n observability apply -f grafana-install/grafana.yaml\n</code></pre> <p>Once deployed, we can access the UI using port-forward, or directly using our Minikube:</p> <pre><code>$ minikube -p kafka service grafana -n observability\n\ud83d\ude3f  service observability/grafana has no node port\n\u2757  Services [observability/grafana] have type \"ClusterIP\" not meant to be exposed, however for local development minikube allows you to access this !\n\ud83c\udfc3  Starting tunnel for service grafana.\n|---------------|---------|-------------|------------------------|\n|   NAMESPACE   |  NAME   | TARGET PORT |          URL           |\n|---------------|---------|-------------|------------------------|\n| observability | grafana |             | http://127.0.0.1:61909 |\n|---------------|---------|-------------|------------------------|\n\ud83c\udf89  Opening service observability/grafana in default browser...\n\u2757  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.\n</code></pre> <p>This will open the browser to the login page of Grafana. The default login/password are : admin/admin.</p> <p>We head to the <code>Configuration &gt; Data Sources</code> tab and add Prometheus as a data source. In the URL field we put the address of our prometheus service : <code>http://prometheus-operated:9090</code>.</p> <p>After <code>Save &amp; Test</code> we should have a green banner indicating that our Prometheus source is up and running.</p> <p>Now is time to add a dashboard in order to visualize our Kafka metrics. In the Dashboard tab, we click on <code>Import</code> and point to our <code>strimzi-kafka.json</code> file. Once imported, the dashboard should look something similar to the following figure:</p> <p></p>"},{"location":"kafka/deploying-kafka-with-strimzi-on-minikube/#generating-some-kafka-events","title":"Generating some Kafka events","text":"<p>At this time, since there is no traffic going on in our Kafka cluster, some panels migh show <code>No Data</code>. To resove this, we will generate some events using Kafka performance tests.</p> <p>First, we create our first topic thanks to our Kafka Operator which is watching for any Kafka CRDs.</p> <pre><code>kubectl apply -f kafka/kafka-topic.yaml\n</code></pre> <p>This will create our first topic <code>my-topic</code> so we can generate some events.</p> <p>Head to the terminal and past the following command:</p> <pre><code>kubectl -n kafka exec -i my-cluster-kafka-0 -c kafka -- \\\n    bin/kafka-producer-perf-test.sh --topic my-topic --num-records 1000000 --record-size 100 --throughput 100 --producer-props bootstrap.servers=my-cluster-kafka-bootstrap:9092 --print-metrics\n501 records sent, 100.2 records/sec (0.01 MB/sec), 8.3 ms avg latency, 301.0 ms max latency.\n501 records sent, 100.1 records/sec (0.01 MB/sec), 1.4 ms avg latency, 8.0 ms max latency.\n500 records sent, 99.9 records/sec (0.01 MB/sec), 1.8 ms avg latency, 35.0 ms max latency.\n501 records sent, 100.0 records/sec (0.01 MB/sec), 1.8 ms avg latency, 39.0 ms max latency.\n500 records sent, 100.0 records/sec (0.01 MB/sec), 1.6 ms avg latency, 8.0 ms max latency.\n...\n</code></pre> <p>Then, we run the consumer with:</p> <pre><code>kubectl -n kafka exec -i my-cluster-kafka-0 -c kafka -- \\\n    bin/kafka-consumer-perf-test.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-topic --from-latest --messages 100000000 --print-metrics --show-detailed-stats\n</code></pre> <p>This will generate some traffic that we can observe on our Grafana dashboards.</p> <p>That\u2019s it. \ud83c\udf89\ud83e\udd73</p> <p>We have created a fully functional Kafka cluster of 3 brokers on a newly created Kubernetes cluster and started monitoring thanks to Prometheus and Grafana.</p> <p>Here is my Github repository containing all the codes for this step-by-step guide.</p> <p>Github repo of this project</p>"},{"location":"observability/getting-started-with-loki-grafana/","title":"Observability","text":"<p>We will setup a monitoring cluster within kubernetes to centralize and process logs. These logs can originate from different sources, from kubernetes it self or other applications deployed outside kubernetes.</p> <p>The stack we will be using consists of Grafana, Loki and Promtail. First, we will use promtail to push logs from different sources to loki Loki will aggregate these logs and store them for further analysis or audit needs Last Grafana will visualize these logs and we can create alerts on some events if needed.</p>"},{"location":"observability/getting-started-with-loki-grafana/#spin-up-a-testing-minikube-cluster","title":"Spin-up a testing minikube cluster","text":"<p>To simplify the demo, we will start from a fresh kubernetes cluster run localy using minikube and we are using Terraform as our Infrastructure As code tool.</p> <pre><code>minikube start --profile observability-cluster\n</code></pre>"},{"location":"observability/getting-started-with-loki-grafana/#use-terraform-for-infrastructure-as-code","title":"Use Terraform for Infrastructure as Code","text":"<p>This ensures that we can bootstrap the cluster quickly in a consistent and declarative way. Also, this same script can be used to deploy this cluster and all resources that we are about to configure on different cloud providers.</p> <p>We create a <code>main.tf</code> file that will hold all our Terraform script. We define the two Terraform providers needed for this setup, <code>Helm</code> and <code>Kubectl</code>. In line number 11, we use our <code>Kubectl</code> provider to create a namespace for our helm charts.</p> <pre><code># Initialize terraform providers\nprovider \"kubernetes\" {\n  config_path = \"~/.kube/config\"\n}\nprovider \"helm\" {\n  kubernetes {\n    config_path = \"~/.kube/config\"\n  }\n}\n# Create a namespace for observability\nresource \"kubernetes_namespace\" \"observability-namespace\" {\n  metadata {\n    name = \"observability\"\n  }\n}\n</code></pre>"},{"location":"observability/getting-started-with-loki-grafana/#bootstrap-grafana","title":"Bootstrap Grafana","text":"<p>Next, we deploy our first Helm chart to install Grafana in the cluster:</p> <pre><code># Helm chart for Grafana\nresource \"helm_release\" \"grafana\" {\n  name             = \"grafana\"\n  repository       = \"https://grafana.github.io/helm-charts\"\n  chart            = \"grafana\"\n  version          = \"7.1.0\"\n  namespace        = \"observability\"\n\n  values = [file(\"${path.module}/values/grafana.yaml\")]\n  depends_on = [ kubernetes_namespace.observability-namespace ]\n}\n</code></pre> <p>We use a value file to configure our Grafana by defining the password for the admin user and, later on, the datasource for Loki. This will help use avoid configuring it each time we redeploy our cluster.</p> <p>Let's go ahead and test our setup so far by typing terrafrom command in the terminal:</p> <pre><code>terraform init\nterraform apply --auto-apply\n</code></pre> <p>This will initialize terraform providers and state and deploy the Grafana helm chart.</p> <p>``` bash title=\"terraform output TODO: put the output here <pre><code>We check what was created on the `observability` namespace and that all the pods are running.\n\n``` bash title=\"kubectl get all --namespace observability\"\nTODO: put the output here\n</code></pre></p> <p>To access the Grafana UI, we need to get access to the cluster. Since we are using a local minikube cluster, the easiest way to access the Grafana service is to create a <code>port-forward</code> using the command. This command forward traffic from our cluster on port 80 to our local machine on port 3000.</p> <pre><code>kubectl port-forward -n observability svc/grafana 3000:80\n</code></pre> <p>We can log into Grafana UI using the <code>admin</code> login and the password defined in the <code>values/grafana.yaml</code>.</p> <p>TODO: Grafana screenshot.</p>"},{"location":"observability/getting-started-with-loki-grafana/#bootstrap-loki","title":"Bootstrap Loki","text":"<p>We do the same with Loki using Terraform too:</p> <pre><code># Helm chart for Loki\nresource \"helm_release\" \"loki\" {\n  name       = \"loki\"\n  repository = \"https://grafana.github.io/helm-charts\"\n  chart      = \"loki\"\n  version    = \"5.41.5\"\n  namespace  = \"observability\"\n\n  values = [file(\"${path.module}/values/loki.yaml\")]\n  depends_on = [ kubernetes_namespace.observability-namespace ]\n}  \n</code></pre> <p>To configure our Loki deployemnt, we create a configuration file under <code>values/loki.yaml</code> and we define our storage settings with the kind of architecture we want to use. Since we are deploying localy for testing only, we use the single binary architecture and the local storage option.</p> <pre><code>loki:\n  auth_enabled: false\n  commonConfig:\n    replication_factor: 1\n  storage:\n    type: 'filesystem'\nsingleBinary:\n  replicas: 1\n\npromtail:\n  enabled: true\n</code></pre>"},{"location":"observability/getting-started-with-loki-grafana/#deploy-and-configure-promtail","title":"Deploy and configure Promtail","text":"<p>Finaly, we deploy Promtail to ship logs to Loki Loki mode of getting logs is different from Prometheus. Prometheus pulls metrics from different targets that are providing metrics endpoints. For logs, Loki relay on push of logs from log shiping tools such as Promtail. We follow the same approach to deploy Promtail on our cluster:</p> <pre><code># Helm chart for promtail\nresource \"helm_release\" \"promtail\" {\n  name       = \"promtail\"\n  repository = \"https://grafana.github.io/helm-charts\"\n  chart      = \"promtail\"\n  version    = \"6.15.3\"\n  namespace  = \"observability\"\n\n  values = [file(\"${path.module}/values/promtail.yaml\")]\n  depends_on = [ kubernetes_namespace.observability-namespace ]\n}\n</code></pre> <p>To configure Promtail, we create a config file <code>values/promtail.yaml</code> with the following content:</p> <pre><code>extraVolumes:\n  - name: node-logs\n    hostPath:\n      path: /var/log\n\nextraVolumeMounts:\n  - name: node-logs\n    mountPath: /var/log/host\n    readOnly: true\n\n# Add Loki as a client to Promtail\nclients:\n  - url: http://loki-gateway.observability.svc.cluster.local/loki/api/v1/push\n</code></pre> <p>This configure Promtail to fetch logs from the current host located in <code>/var/log/host</code> and ship them to Loki endpoint.</p> <p>And that's it, we have put all the pieces necessary for collecting logs from Kubernetes and shiping them to Grafana for further analysis and audits.</p> <p>\\(TODO: Put the necessary screenshot\\)</p>"},{"location":"observability/getting-started-with-loki-grafana/#video-tutorial","title":"Video tutorial","text":"<p>If you prefer to follow along, this tutorial is available here:</p>"},{"location":"observability/monitoring-k8s-audit-logs-with-grafana/","title":"Monitoring K8s audit logs with Loki, Grafana &amp; Prometheus","text":"<p>In this tutorial, we will monitor Kubernetes audit logs using the Grafana, Loki, Promtail and Prometheus stack. The goal is to quickly bootstrap a security-monitored Kubernetes cluster using Kubernetes vanilla audit capability. Our setup is described in the figure bellew:</p> <p> TODO: Add architecture diagram here</p>"},{"location":"observability/monitoring-k8s-audit-logs-with-grafana/#configure-kubernetes-audit-policy","title":"Configure Kubernetes Audit Policy","text":"<p>We start by creating a new Kubernetes cluster using Minikube so that you can redo this tutorial and test the setup before trying it on a cloud deployed cluster.</p> <pre><code>$ minikube start\n\ud83d\ude04  minikube v1.32.0 on Darwin 14.2.1 (arm64)\n\u2728  Using the docker driver based on existing profile\n\ud83d\udc4d  Starting control plane node minikube in cluster minikube\n\ud83d\ude9c  Pulling base image ...\n\ud83d\udc33  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...\n\ud83d\udd17  Configuring bridge CNI (Container Networking Interface) ...\n\ud83d\udd0e  Verifying Kubernetes components...\n    \u25aa Using image gcr.io/k8s-minikube/storage-provisioner:v5\n\ud83c\udf1f  Enabled addons: storage-provisioner, default-storageclass\n\ud83c\udfc4  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n</code></pre> <p>Once the cluster started, we login into it using <code>minikube ssh</code> so that we can configure the <code>kube-apiserver</code> with an <code>audit-policy</code> and how audit logs should be stored.</p> <p>To enable audit logs on a Minikube:</p>"},{"location":"observability/monitoring-k8s-audit-logs-with-grafana/#1-configure-kube-apiserver","title":"1. Configure Kube-apiserver","text":"<p>Using the official kubernetes documentation as reference, we login into our minikube VM and configure the <code>kube-apiserver</code> as follow:</p> <pre><code>$ minikube ssh\n# we create a backup copy of the kube-apiserver manifest file in case we mess things up \ud83d\ude04\ndocker@minikube:~$ sudo cp /etc/kubernetes/manifests/kube-apiserver.yaml .\n# edit the file to add audit logs configurations\ndocker@minikube:~$ sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>We need to instruct the <code>kube-apiserver</code> to start using an <code>audit-policy</code> that will define what logs we want to capture, then where to which file we want to send them. This is done by adding these two lines bellow the kube-apiserver command:</p> <pre><code>  - command:\n    - kube-apiserver\n    # add the following two lines\n    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml\n    - --audit-log-path=/var/log/kubernetes/audit/audit.log\n    # end \n    - --advertise-address=192.168.49.2\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n</code></pre> <p>With both files, we need need to configure the <code>volumes</code> and <code>volumeMount</code> into the <code>kube-apiserver</code> container. Scroll down into the same file and add these lines:</p> <pre><code>...\nvolumeMounts:\n  - mountPath: /etc/kubernetes/audit-policy.yaml\n    name: audit\n    readOnly: true\n  - mountPath: /var/log/kubernetes/audit/\n    name: audit-log\n    readOnly: false\n</code></pre> <p>And then:</p> <pre><code>...\nvolumes:\n- name: audit\n  hostPath:\n    path: /etc/kubernetes/audit-policy.yaml\n    type: File\n\n- name: audit-log\n  hostPath:\n    path: /var/log/kubernetes/audit/\n    type: DirectoryOrCreate\n</code></pre> <p>Be carefull with the number of spaces you add before each line, this can prevent the <code>kube-apiserver</code> from starting.</p> <p>However, at this point, even if you are super carefull, it wont start... \ud83d\ude08</p>"},{"location":"observability/monitoring-k8s-audit-logs-with-grafana/#2-create-the-audit-policy","title":"2. Create the audit-policy","text":"<p>This is because, we need to create the audit-policy file at the location we gave to the <code>kube-apiserver</code>. To keep things simple, we will use the audit-policy provided by the kubernetes documentation.</p> <pre><code>docker@minikube:~$ cd /etc/kubernetes/\ndocker@minikube:~$ sudo curl -sLO https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/audit/audit-policy.yaml\n</code></pre> <p>Now, everything should be Ok for the <code>kuber-apiserver</code> pod to come back to a running state.</p> <pre><code>docker@minikube:~$ exit\n$ kubectl get po -n kube-system\nNAME                               READY   STATUS    RESTARTS   AGE\ncoredns-5dd5756b68-mbtm8           1/1     Running   0          1h\netcd-minikube                      1/1     Running   0          1h\nkube-apiserver-minikube            1/1     Running   0          13s\nkube-controller-manager-minikube   1/1     Running   0          1h\nkube-proxy-jcn6v                   1/1     Running   0          1h\nkube-scheduler-minikube            1/1     Running   0          1h\nstorage-provisioner                1/1     Running   0          1h \n</code></pre>"},{"location":"observability/monitoring-k8s-audit-logs-with-grafana/#3-check-audit-logs-are-generated","title":"3. Check audit logs are generated","text":"<p>Now, we can log back to the Minikube VM to check that our audit logs are correctly generated in the provided audit.log file.</p> <pre><code>$ minikube ssh\ndocker@minikube:~$ sudo cat /var/log/kubernetes/audit/audit.log\n...\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Metadata\", ...\n\"authorization.k8s.io/reason\":\"RBAC:\nallowed by ClusterRoleBinding \\\"system:public-info-viewer\\\" of ClusterRole\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1\",\"level\":\"Request\", ...\n...\n</code></pre> <p>Yes, we have our logs. \ud83e\udd73</p>"},{"location":"observability/monitoring-k8s-audit-logs-with-grafana/#deploying-the-monitoring-stack","title":"Deploying the monitoring stack","text":"<p>Now we need to setup the monitoring stack and push these logs to Grafana.</p> <p>The stack we will be using consists of Grafana, Loki and Promtail. First, we will use promtail to push logs from within our cluster node to loki. Loki will aggregate these logs and store them for further analysis or audit needs. Last Grafana will visualize these logs during assessments, investigations and potentially create alerts on some usecases.</p>"},{"location":"observability/monitoring-k8s-audit-logs-with-grafana/#power-up-with-infrastructure-as-code","title":"Power-up with Infrastructure as Code","text":"<p>Since I already published and article on how to setup a similar monitoring stack, you can refer to my previous article for more details. For now, just create this terraform file and run <code>terraform apply</code> in the same folder to do the trick:</p> <pre><code># Initialize terraform providers\nprovider \"kubernetes\" {\n  config_path = \"~/.kube/config\"\n}\nprovider \"helm\" {\n  kubernetes {\n    config_path = \"~/.kube/config\"\n  }\n}\n# Create a namespace for observability\nresource \"kubernetes_namespace\" \"observability-namespace\" {\n  metadata {\n    name = \"observability\"\n  }\n}\n# Helm chart for Grafana\nresource \"helm_release\" \"grafana\" {\n  name       = \"grafana\"\n  repository = \"https://grafana.github.io/helm-charts\"\n  chart      = \"grafana\"\n  version    = \"7.1.0\"\n  namespace  = \"observability\"\n\n  values     = [file(\"${path.module}/values/grafana.yaml\")]\n  depends_on = [kubernetes_namespace.observability-namespace]\n}\n# Helm chart for Loki\nresource \"helm_release\" \"loki\" {\n  name       = \"loki\"\n  repository = \"https://grafana.github.io/helm-charts\"\n  chart      = \"loki\"\n  version    = \"5.41.5\"\n  namespace  = \"observability\"\n\n  values     = [file(\"${path.module}/values/loki.yaml\")]\n  depends_on = [kubernetes_namespace.observability-namespace]\n}\n# Helm chart for promtail\nresource \"helm_release\" \"promtail\" {\n  name       = \"promtail\"\n  repository = \"https://grafana.github.io/helm-charts\"\n  chart      = \"promtail\"\n  version    = \"6.15.3\"\n  namespace  = \"observability\"\n\n  values     = [file(\"${path.module}/values/promtail.yaml\")]\n  depends_on = [kubernetes_namespace.observability-namespace]\n}\n</code></pre> <p>For this script to run properly, you need to create a <code>values</code> folder along with the <code>terraform</code> file.</p> <p>This folder will hold all our configuration for the stack to get the audit logs into Grafana.</p> <p>Here is the content for each file:</p> values/grafana.yaml<pre><code>persistence.enabled: true\npersistence.size: 10Gi\npersistence.existingClaim: grafana-pvc\npersistence.accessModes[0]: ReadWriteOnce\npersistence.storageClassName: standard\n\nadminUser: admin\nadminPassword: grafana\n\ndatasources: \n datasources.yaml:\n   apiVersion: 1\n   datasources:\n    - name: Loki\n      type: loki\n      access: proxy\n      orgId: 1\n      url: http://loki-gateway.observability.svc.cluster.local\n      basicAuth: false\n      isDefault: true\n      version: 1\n</code></pre> values/loki.yaml<pre><code>loki:\n  auth_enabled: false\n  commonConfig:\n    replication_factor: 1\n  storage:\n    type: 'filesystem'\nsingleBinary:\n  replicas: 1\n</code></pre> values/promtail.yaml<pre><code># Add Loki as a client to Promtail\nconfig:\n  clients:\n    - url: http://loki-gateway.observability.svc.cluster.local/loki/api/v1/push\n\n\n# Scraping kubernetes audit logs located in /var/log/kubernetes/audit/\n  snippets:\n    scrapeConfigs: |\n      - job_name: audit-logs\n        static_configs:\n          - targets:\n              - localhost\n            labels:\n              job: audit-logs\n              __path__: /var/log/host/kubernetes/**/*.log\n</code></pre> <p>Now, everything is setup for our monitoring stack to be able to ingest audit logs from our kubernetes node.</p> <p>We can log into our Grafana using the credentials configured in the value file above.</p> <p>We head to the Explore tab and check our audit logs.</p>"},{"location":"observability/monitoring-k8s-audit-logs-with-grafana/#video-tutorial","title":"Video tutorial","text":"<p>If you prefer to follow along, this tutorial is available here:</p>"},{"location":"other/examples/","title":"Examples","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"other/examples/#code-annotation-examples","title":"Code Annotation Examples","text":""},{"location":"other/examples/#codeblocks","title":"Codeblocks","text":"<p>Some <code>code</code> goes here.</p>"},{"location":"other/examples/#plain-codeblock","title":"Plain codeblock","text":"<p>A plain codeblock:</p> <pre><code>Some code here\ndef myfunction()\n// some comment\n</code></pre>"},{"location":"other/examples/#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start:</p> <pre><code>import tensorflow as tf\ndef whatever()\n</code></pre>"},{"location":"other/examples/#with-a-title","title":"With a title","text":"bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"other/examples/#with-line-numbers","title":"With line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"other/examples/#highlighting-lines","title":"Highlighting lines","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"other/examples/#icons-and-emojs","title":"Icons and Emojs","text":""},{"location":"other/examples/#images","title":"Images","text":"<p> Format: </p>"},{"location":"other/examples/#links","title":"Links","text":"<p>https://github.com - automatic! GitHub</p>"}]}